# Model Profiles & Orchestration

This document describes the unified model registry and orchestration system used by both the **Web Dashboard** and **Jexida CLI**.

## Overview

The Jexida platform supports multiple AI models from different sources:

- **External Models**: Cloud-based APIs (OpenAI GPT-5, O-Series, GPT-4.1)
- **Local Models**: Self-hosted via Ollama (llama3, phi3, etc.)

Both the web UI and CLI share a **central registry** and can select models or orchestration strategies uniformly.

## Key Concepts

### Model Profile

A **ModelProfile** defines a single AI model with its capabilities and configuration:

```python
ModelProfile(
    id="gpt-5-nano",                    # Unique identifier
    display_name="GPT-5 Nano",          # Human-readable name
    source=ModelSource.EXTERNAL,        # LOCAL or EXTERNAL
    provider=ModelProvider.OPENAI,      # OLLAMA, OPENAI, AZURE_OPENAI
    model_id="gpt-5-nano",              # API model identifier
    group="ğŸš€ GPT-5 Series (Latest)",   # UI grouping
    tier=ModelTier.BUDGET,              # BUDGET, STANDARD, PREMIUM, FLAGSHIP
    supports_temperature=False,          # Whether temp param works
    supports_tools=True,                 # Function calling support
    ...
)
```

### Strategy

A **ModelStrategy** wraps one or more models for orchestration:

| Strategy Type | Description | Example |
|--------------|-------------|---------|
| `single` | Direct 1:1 mapping to a model | `single:gpt-5-nano` |
| `cascade` | Ordered list, try cheap first | `cascade:cloud-cheapest-first` |
| `router` | (Future) Classification-based | `router:task-based` |

### Cascade Strategies

Cascades try models in order until one succeeds:

```
cascade:cloud-cheapest-first
  â†“ gpt-5-nano (try first - cheapest)
  â†“ gpt-5-mini (escalate if needed)
  â†“ gpt-5     (full power)
  â†“ o1        (ultimate reasoning)
```

## Configuration

### External Models (OpenAI)

Configure in `.env`:

```bash
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-5-nano  # Default model
```

### Local Models (Ollama)

1. Ensure Ollama is running on your MCP server
2. Use the discover endpoint or CLI command to find models:

**Web API:**
```bash
curl -X POST "http://localhost:8080/api/assistant/strategies/discover-local?ollama_host=http://localhost:11434"
```

**CLI:**
```
JEXIDA> /model
```

## API Endpoints

### List Strategies

```
GET /api/assistant/strategies
```

Returns:
```json
{
  "strategies": [...],
  "active_strategy_id": "single:gpt-5-nano",
  "groups": [
    "ğŸ”€ Auto / Orchestration",
    "ğŸš€ GPT-5 Series (Latest)",
    "ğŸ§  O-Series (Reasoning)",
    "â­ GPT-4 Series",
    "ğŸ–¥ï¸ Local Models"
  ]
}
```

### Get Active Strategy

```
GET /api/assistant/strategies/active
```

### Set Active Strategy

```
POST /api/assistant/strategies/active?strategy_id=cascade:cloud-cheapest-first
```

### Discover Local Models

```
POST /api/assistant/strategies/discover-local?ollama_host=http://192.168.1.224:11434
```

## Web Dashboard Usage

### Select a Model

1. Open the **Assistant** page
2. Click the **Model** dropdown
3. Choose from:
   - **ğŸ”€ Auto / Orchestration**: Cascade strategies
   - **ğŸš€ GPT-5 Series**: Latest OpenAI models
   - **ğŸ§  O-Series**: Reasoning-focused models
   - **â­ GPT-4 Series**: Previous generation
   - **ğŸ–¥ï¸ Local Models**: Ollama models (after discovery)

### Temperature Control

- Shows only for models that support it (GPT-4 series)
- Hidden for GPT-5, O-series, and cascade strategies
- Slider: 0 (precise) â†’ 2 (creative)

## CLI Usage

### List Available Models & Strategies

```
JEXIDA> /model
```

Shows grouped list with current selection marked.

### Switch to a Model

```
JEXIDA> /model gpt-5-nano
```

Or with full strategy ID:

```
JEXIDA> /model set single:gpt-4.1
```

### Switch to a Cascade Strategy

```
JEXIDA> /model set cascade:cloud-cheapest-first
JEXIDA> /model set cascade:local-first
JEXIDA> /model set cascade:reasoning
```

## Example Flows

### Web: Use Auto-Cascade

1. Open Assistant page
2. Select "Auto â€” Cheapest First (Cloud)"
3. Send a query
4. System tries `gpt-5-nano` first, escalates if needed

### CLI: Use Local-First

```
JEXIDA> /model set cascade:local-first
ğŸ”„ Strategy changed to Auto â€” Local First

JEXIDA> Plan a multi-step Azure deployment...
[Uses local model first, falls back to cloud if needed]
```

### Web: Use Specific Model

1. Select "GPT-4.1 â€” Premium Â· Temp: âœ“"
2. Adjust temperature slider (0.3 for precise, 0.9 for creative)
3. Send query

## Built-in Strategies

| Strategy ID | Display Name | Models |
|------------|--------------|--------|
| `cascade:cloud-cheapest-first` | Auto â€” Cheapest First (Cloud) | gpt-5-nano â†’ gpt-5-mini â†’ gpt-5 â†’ o1 |
| `cascade:local-first` | Auto â€” Local First | [local models] â†’ gpt-5-nano â†’ gpt-5-mini |
| `cascade:reasoning` | Auto â€” Reasoning Focus | o3-mini â†’ o4-mini â†’ o1 |
| `single:gpt-5-nano` | GPT-5 Nano | gpt-5-nano only |
| `single:local:llama3:latest` | Llama 3 | llama3:latest only |

## Adding Custom Strategies

In `unified_registry.py`:

```python
registry.create_cascade_strategy(
    strategy_id="cascade:my-custom",
    display_name="My Custom Cascade",
    model_ids=["local:phi3:latest", "gpt-5-nano", "gpt-5"],
    description="Local first, then cloud escalation",
    group="ğŸ”€ Auto / Orchestration",
)
```

## Model Groups (UI)

| Group | Icon | Description |
|-------|------|-------------|
| Auto / Orchestration | ğŸ”€ | Cascade and router strategies |
| GPT-5 Series (Latest) | ğŸš€ | Current gen OpenAI models |
| O-Series (Reasoning) | ğŸ§  | Reasoning-focused models |
| GPT-4 Series | â­ | Previous gen (with temp support) |
| Local Models | ğŸ–¥ï¸ | Ollama/self-hosted models |

## Tier Labels

| Tier | Label | Description |
|------|-------|-------------|
| flagship | Most Capable | Maximum performance |
| premium | Premium | High capability |
| standard | Balanced | Good balance |
| budget | Fast & Cheap | Cost-efficient |

## Troubleshooting

### Local models not showing

1. Ensure Ollama is running: `ollama serve`
2. Call discover endpoint with correct host
3. Check server logs for connection errors

### Strategy not found

- Use `/model` to list valid strategy IDs
- Strategy IDs have format: `type:identifier`
  - `single:model-id`
  - `cascade:strategy-name`

### Temperature not working

- Only GPT-4.x series supports temperature
- GPT-5 and O-series use fixed temperature (1.0)
- Cascade strategies hide temp slider

